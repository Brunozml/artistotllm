{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt/LuOlgvZa19ujDh/cBr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brunozml/artistotllm/blob/main/n_gram_similarity_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JQusH6gfis",
        "outputId": "4deff135-b161-4d48-a4fd-0490e3ba956a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: gpt_what_to_do.txt\n",
            "Text 2: hypewrite_what_to_do.txt\n",
            "\n",
            "N-gram Similarity score: 0.57\n",
            "\n",
            "Top weighted n-grams in Text 1:\n",
            "'of': 0.229\n",
            "'your': 0.229\n",
            "'and': 0.261\n",
            "'to': 0.261\n",
            "'the': 0.457\n",
            "\n",
            "Top weighted n-grams in Text 2:\n",
            "'do': 0.186\n",
            "'what': 0.256\n",
            "'to': 0.279\n",
            "'you': 0.341\n",
            "'the': 0.419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "🎯 N-gram Navigator: Your Text's Best Friend! 🎯\n",
        "\n",
        "Welcome to the N-gram similarity analyzer, where we slice and dice text into delicious\n",
        "bite-sized chunks (n-grams) and compare them using the mighty TF-IDF powers! It's like\n",
        "a word sandwich detector that tells you how similar two texts are based on their\n",
        "ingredient combinations.\n",
        "\n",
        "Pro tip: Works best with a cup of coffee and a sense of humor! ☕️🔍\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from typing import Tuple, List, Set\n",
        "import requests # Import the requests library\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the specific resource needed by pos_tag for English\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_ngrams(text: str, n: int) -> List[str]:\n",
        "    \"\"\"Generate n-grams from text\"\"\"\n",
        "    words = text.lower().split()\n",
        "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "def calculate_tfidf(texts: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Calculate TF-IDF vectors for the texts\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "def ngram_similarity(text1: str, text2: str, n: int = 2) -> Tuple[float, Dict]:\n",
        "    \"\"\"\n",
        "    Compare two texts using n-gram overlap with TF-IDF weights.\n",
        "    Returns a similarity score and the n-grams with their TF-IDF weights.\n",
        "    \"\"\"\n",
        "    # Get n-grams\n",
        "    ngrams1 = get_ngrams(text1, n)\n",
        "    ngrams2 = get_ngrams(text2, n)\n",
        "\n",
        "    # Join n-grams back to text for TF-IDF calculation\n",
        "    text1_ngrams = ' '.join(ngrams1)\n",
        "    text2_ngrams = ' '.join(ngrams2)\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf_matrix, feature_names = calculate_tfidf([text1_ngrams, text2_ngrams])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = np.dot(tfidf_matrix[0], tfidf_matrix[1]) / (\n",
        "        np.linalg.norm(tfidf_matrix[0]) * np.linalg.norm(tfidf_matrix[1])\n",
        "    )\n",
        "\n",
        "    # Get top weighted n-grams for each text\n",
        "    def get_top_ngrams(tfidf_vector, feature_names, top_n=5):\n",
        "        indices = np.argsort(tfidf_vector)[-top_n:]\n",
        "        return {feature_names[i]: float(tfidf_vector[i]) for i in indices if tfidf_vector[i] > 0}\n",
        "\n",
        "    return similarity, {\n",
        "        \"text1_top_ngrams\": get_top_ngrams(tfidf_matrix[0], feature_names),\n",
        "        \"text2_top_ngrams\": get_top_ngrams(tfidf_matrix[1], feature_names)\n",
        "    }\n",
        "\n",
        "def read_file(filepath):\n",
        "    \"\"\"Read text from a file (local or URL) and return its contents\"\"\"\n",
        "    # Check if the filepath is a URL\n",
        "    if filepath.startswith('http://') or filepath.startswith('https://'):\n",
        "        try:\n",
        "            response = requests.get(filepath)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
        "            return response.text\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching URL {filepath}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # Assume it's a local file path\n",
        "        try:\n",
        "            with open(filepath, 'r') as f:\n",
        "                return f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error opening local file {filepath}: File not found\")\n",
        "            return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example texts\n",
        "    # text1 = \"The cat quickly jumped over the lazy dog.\"\n",
        "    # text2 = \"A dog slowly walked under the tired cat.\"\n",
        "    # Read the files\n",
        "    data_path = 'https://raw.githubusercontent.com/Brunozml/artistotllm/main/data/raw/' # Use raw.githubusercontent.com for direct file access\n",
        "    file1 = 'gpt_what_to_do.txt'\n",
        "    file2 = 'hypewrite_what_to_do.txt'\n",
        "\n",
        "    # Use the updated read_file function\n",
        "    text1 = read_file(data_path + file1)\n",
        "    text2 = read_file(data_path + file2)\n",
        "\n",
        "    # Only proceed if both files were read successfully\n",
        "    if text1 is not None and text2 is not None:\n",
        "        # Compare texts\n",
        "        similarity_score, gram_info = ngram_similarity(text1, text2)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Text 1: {file1}\")\n",
        "        print(f\"Text 2: {file2}\")\n",
        "\n",
        "        print(f\"\\nN-gram Similarity score: {similarity_score:.2f}\")\n",
        "        print(\"\\nTop weighted n-grams in Text 1:\")\n",
        "        for ngram, weight in gram_info[\"text1_top_ngrams\"].items():\n",
        "            print(f\"'{ngram}': {weight:.3f}\")\n",
        "        print(\"\\nTop weighted n-grams in Text 2:\")\n",
        "        for ngram, weight in gram_info[\"text2_top_ngrams\"].items():\n",
        "            print(f\"'{ngram}': {weight:.3f}\")"
      ]
    }
  ]
}