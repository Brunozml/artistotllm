{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOXPr5VBF124uWyawVsbdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brunozml/artistotllm/blob/main/pos_similarity_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWtNUSiReR02",
        "outputId": "794e6a0f-1461-4764-a4d7-e213cacbc343"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from typing import Tuple, List, Set\n",
        "import requests # Import the requests library\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the specific resource needed by pos_tag for English\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_pos_distribution(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get the distribution of POS tags in the text.\n",
        "    Returns a dictionary with POS tags as keys and their frequencies as values.\n",
        "    \"\"\"\n",
        "    # Tokenize and get POS tags\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Count POS tag frequencies\n",
        "    pos_dist = {}\n",
        "    for _, tag in pos_tags:\n",
        "        pos_dist[tag] = pos_dist.get(tag, 0) + 1\n",
        "\n",
        "    # Normalize frequencies\n",
        "    total = sum(pos_dist.values())\n",
        "    if total == 0: # Handle empty text case to avoid division by zero\n",
        "        return {}\n",
        "    for tag in pos_dist:\n",
        "        pos_dist[tag] = pos_dist[tag] / total\n",
        "\n",
        "    return pos_dist\n",
        "\n",
        "def pos_similarity(text1: str, text2: str) -> Tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    Compare two texts based on their POS tag distributions.\n",
        "    Returns a similarity score and the POS distributions.\n",
        "    \"\"\"\n",
        "    # Get POS distributions\n",
        "    dist1 = get_pos_distribution(text1)\n",
        "    dist2 = get_pos_distribution(text2)\n",
        "\n",
        "    # Get all unique POS tags\n",
        "    all_tags = set(dist1.keys()) | set(dist2.keys())\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    vec1 = np.array([dist1.get(tag, 0) for tag in all_tags])\n",
        "    vec2 = np.array([dist2.get(tag, 0) for tag in all_tags])\n",
        "\n",
        "    # Handle cases where one or both vectors are zero vectors (e.g., empty texts)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        similarity = 0.0\n",
        "    else:\n",
        "        similarity = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
        "\n",
        "    return similarity, {\"text1_pos\": dist1, \"text2_pos\": dist2}\n",
        "\n",
        "def read_file(filepath):\n",
        "    \"\"\"Read text from a file (local or URL) and return its contents\"\"\"\n",
        "    if filepath.startswith('http://') or filepath.startswith('https://'):\n",
        "        # If it's a URL, use requests to fetch the content\n",
        "        try:\n",
        "            response = requests.get(filepath)\n",
        "            response.raise_for_status() # Raise an exception for bad status codes\n",
        "            return response.text\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching URL {filepath}: {e}\")\n",
        "            return None # Or raise the exception\n",
        "    else:\n",
        "        # Otherwise, treat it as a local file path\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f: # Added encoding for wider compatibility\n",
        "                return f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Local file not found at {filepath}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading local file {filepath}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example texts\n",
        "    # text1 = \"The cat quickly jumped over the lazy dog.\"\n",
        "    # text2 = \"A dog slowly walked under the tired cat.\"\n",
        "    # Read the files\n",
        "    # Construct the full URLs for the raw files\n",
        "    data_path = 'https://raw.githubusercontent.com/Brunozml/artistotllm/main/data/raw/' # Use raw.githubusercontent.com for direct file access\n",
        "    file1 = 'gpt_what_to_do.txt'\n",
        "    file2 = 'hypewrite_what_to_do.txt'\n",
        "\n",
        "    text1 = read_file(data_path + file1)\n",
        "    text2 = read_file(data_path + file2)\n",
        "\n",
        "    # Only proceed if both texts were successfully read\n",
        "    if text1 is not None and text2 is not None:\n",
        "        # Compare texts\n",
        "        similarity_score, pos_distributions = pos_similarity(text1, text2)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Text 1: {file1}\")\n",
        "        print(f\"Text 2: {file2}\")\n",
        "        print(f\"\\nPOS Similarity score: {similarity_score:.2f}\")\n",
        "        # print(\"\\nPOS Distribution Text 1:\")\n",
        "        # for pos, freq in pos_distributions[\"text1_pos\"].items():\n",
        "        #     print(f\"{pos}: {freq:.2f}\")\n",
        "        # print(\"\\nPOS Distribution Text 2:\")\n",
        "        # for pos, freq in pos_distributions[\"text2_pos\"].items():\n",
        "        #     print(f\"{pos}: {freq:.2f}\")\n",
        "    else:\n",
        "        print(\"Could not read one or both input files. Exiting.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STSH9-bIe7Wv",
        "outputId": "c9d3dec1-408c-4573-fdec-21df9d839dce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: gpt_what_to_do.txt\n",
            "Text 2: hypewrite_what_to_do.txt\n",
            "\n",
            "POS Similarity score: 0.88\n"
          ]
        }
      ]
    }
  ]
}