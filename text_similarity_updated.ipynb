{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUAS28oTiI7uGxnoxgIbFN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brunozml/artistotllm/blob/main/text_similarity_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN73R3JQihhL",
        "outputId": "301f3fc9-bc9a-4f51-f430-f72d62a5d7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: hypewrite_what_to_do.txt\n",
            "Text 2: gpt_what_to_do.txt\n",
            "\n",
            "Jaccard Similarity score: 0.21\n",
            "Cosine Similarity score: 0.66\n",
            "\n",
            "Number of shared words: 66\n",
            "Sample of shared words (up to 10): ['treat', 'and', 'this', 'isn', 't', 'now', 'a', 'learning', 'doing', 'something']\n",
            "Text 1 unique words: 226\n",
            "Text 2 unique words: 158\n",
            "Total unique words across both texts: 318\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from typing import Tuple, Set\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "def get_word_set(text: str) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Extract unique words from text after preprocessing.\n",
        "    Returns a set of lowercase words with punctuation removed.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase and extract words, removing punctuation\n",
        "    words = set(re.findall(r'\\w+', text.lower()))\n",
        "    return words\n",
        "\n",
        "def get_word_frequency(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get word frequency distribution in the text.\n",
        "    Returns a dictionary with words as keys and their frequencies as values.\n",
        "    \"\"\"\n",
        "    # Extract words and count frequencies\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Normalize frequencies\n",
        "    total = sum(word_freq.values())\n",
        "    if total == 0:  # Handle empty text case to avoid division by zero\n",
        "        return {}\n",
        "\n",
        "    normalized_freq = {word: count / total for word, count in word_freq.items()}\n",
        "    return normalized_freq\n",
        "\n",
        "def jaccard_similarity(text1: str, text2: str) -> Tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    Compare two texts using Jaccard similarity (word overlap approach).\n",
        "    Returns a similarity score between 0 and 1 and additional metrics.\n",
        "    \"\"\"\n",
        "    # Get word sets\n",
        "    words1 = get_word_set(text1)\n",
        "    words2 = get_word_set(text2)\n",
        "\n",
        "    # Find intersection and union\n",
        "    intersection = words1.intersection(words2)\n",
        "    union = words1.union(words2)\n",
        "\n",
        "    # Calculate Jaccard similarity\n",
        "    if len(union) == 0:  # Handle empty texts\n",
        "        similarity = 0.0\n",
        "    else:\n",
        "        similarity = len(intersection) / len(union)\n",
        "\n",
        "    return similarity, {\n",
        "        \"shared_words\": intersection,\n",
        "        \"total_unique_words\": len(union),\n",
        "        \"text1_unique_words\": len(words1),\n",
        "        \"text2_unique_words\": len(words2)\n",
        "    }\n",
        "\n",
        "def cosine_similarity(text1: str, text2: str) -> Tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    Compare two texts using cosine similarity based on word frequencies.\n",
        "    Returns a similarity score between 0 and 1 and frequency distributions.\n",
        "    \"\"\"\n",
        "    # Get word frequency distributions\n",
        "    freq1 = get_word_frequency(text1)\n",
        "    freq2 = get_word_frequency(text2)\n",
        "\n",
        "    # Get all unique words\n",
        "    all_words = set(freq1.keys()) | set(freq2.keys())\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vec1 = np.array([freq1.get(word, 0) for word in all_words])\n",
        "    vec2 = np.array([freq2.get(word, 0) for word in all_words])\n",
        "\n",
        "    # Handle cases where one or both vectors are zero vectors\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        similarity = 0.0\n",
        "    else:\n",
        "        similarity = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
        "\n",
        "    return similarity, {\"text1_freq\": freq1, \"text2_freq\": freq2}\n",
        "\n",
        "def text_similarity(text1: str, text2: str, method: str = \"jaccard\") -> Tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    Compare two texts based on the specified similarity method.\n",
        "    Available methods: 'jaccard', 'cosine'\n",
        "    Returns a similarity score and additional metrics.\n",
        "    \"\"\"\n",
        "    if method.lower() == \"jaccard\":\n",
        "        return jaccard_similarity(text1, text2)\n",
        "    elif method.lower() == \"cosine\":\n",
        "        return cosine_similarity(text1, text2)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}. Available methods: 'jaccard', 'cosine'\")\n",
        "\n",
        "def read_file(filepath):\n",
        "    \"\"\"Read text from a file (local or URL) and return its contents\"\"\"\n",
        "    if filepath.startswith('http://') or filepath.startswith('https://'):\n",
        "        # If it's a URL, use requests to fetch the content\n",
        "        try:\n",
        "            response = requests.get(filepath)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "            return response.text\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching URL {filepath}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # Otherwise, treat it as a local file path\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Local file not found at {filepath}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading local file {filepath}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read the files\n",
        "    # Construct the full URLs for the raw files\n",
        "    data_path = 'https://raw.githubusercontent.com/Brunozml/artistotllm/main/data/raw/'\n",
        "    file1 = 'hypewrite_what_to_do.txt'\n",
        "    file2 = 'gpt_what_to_do.txt'\n",
        "\n",
        "    text1 = read_file(data_path + file1)\n",
        "    text2 = read_file(data_path + file2)\n",
        "\n",
        "    # Only proceed if both texts were successfully read\n",
        "    if text1 is not None and text2 is not None:\n",
        "        # Compare texts using both methods\n",
        "        jaccard_score, jaccard_metrics = text_similarity(text1, text2, method=\"jaccard\")\n",
        "        cosine_score, cosine_metrics = text_similarity(text1, text2, method=\"cosine\")\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Text 1: {file1}\")\n",
        "        print(f\"Text 2: {file2}\")\n",
        "        print(f\"\\nJaccard Similarity score: {jaccard_score:.2f}\")\n",
        "        print(f\"Cosine Similarity score: {cosine_score:.2f}\")\n",
        "        print(f\"\\nNumber of shared words: {len(jaccard_metrics['shared_words'])}\")\n",
        "        print(f\"Sample of shared words (up to 10): {list(jaccard_metrics['shared_words'])[:10]}\")\n",
        "        print(f\"Text 1 unique words: {jaccard_metrics['text1_unique_words']}\")\n",
        "        print(f\"Text 2 unique words: {jaccard_metrics['text2_unique_words']}\")\n",
        "        print(f\"Total unique words across both texts: {jaccard_metrics['total_unique_words']}\")\n",
        "    else:\n",
        "        print(\"Could not read one or both input files. Exiting.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_texts(text1, text2):\n",
        "    \"\"\"\n",
        "    Compare similarity between two texts using a simple word overlap approach.\n",
        "    Returns a similarity score between 0 and 1.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    words1 = set(re.findall(r'\\w+', text1.lower()))\n",
        "    words2 = set(re.findall(r'\\w+', text2.lower()))\n",
        "\n",
        "    # Find common words\n",
        "    common_words = words1.intersection(words2)\n",
        "\n",
        "    # Calculate similarity score\n",
        "    similarity = len(common_words) / max(len(words1), len(words2))\n",
        "\n",
        "    return similarity, common_words\n",
        "\n",
        "# Manually input your texts here\n",
        "text1 = \"\"\"The word \"prig\" isn't very common now, but if you look up the definition, it will sound familiar. Google's isn't bad: A self-righteously moralistic person who behaves as if superior to others. This sense of the word originated in the 18th century, and its age is an important clue: it shows that although wokeness is a comparatively recent phenomenon, it's an instance of a much older one.There's a certain kind of person who's attracted to a shallow, exacting kind of moral purity, and who demonstrates his purity by attacking anyone who breaks the rules. Every society has these people. All that changes is the rules they enforce. In Victorian England it was Christian virtue. In Stalin's Russia it was orthodox Marxism-Leninism. For the woke, it's social justice.So if you want to understand wokeness, the question to ask is not why people behave this way. Every society has prigs. The question to ask is why our prigs are priggish about these ideas, at this moment. And to answer that we have to ask when and where wokeness began.The answer to the first question is the 1980s. Wokeness is a second, more aggressive wave of political correctness, which started in the late 1980s, died down in the late 1990s, and then returned with a vengeance in the early 2010s, finally peaking after the riots of 2020.This was not the original meaning of \"woke,\" but it's rarely used in the original sense now. Now the pejorative sense is the dominant one. What does it mean now? I've often been asked to define both wokeness and political correctness by people who think they're meaningless labels, so I will. They both have the same definition: An aggressively performative focus on social justice. In other words, it's people being prigs about social justice. And that's the real problem — the performativeness, not the social justice.Racism, for example, is a genuine problem. Not a problem on the scale that the woke believe it to be, but a genuine one. I don't think any reasonable person would deny that. The problem with political correctness was not that it focused on marginalized groups, but the shallow, aggressive way in which it did so. Instead of going out into the world and quietly helping members of marginalized groups, the politically correct focused on getting people in trouble for using the wrong words to talk about them.As for where political correctness began, if you think about it, you probably already know the answer. Did it begin outside universities and spread to them from this external source? Obviously not; it has always been most extreme in universities. So where in universities did it begin? Did it begin in math, or the hard sciences, or engineering, and spread from there to the humanities and social sciences? Those are amusing images, but no, obviously it began in the humanities and social sciences.Why there? And why then? What happened in the humanities and social sciences in the 1980s?A successful theory of the origin of\"\"\"\n",
        "text2 = \"\"\"wokeness would need to explain why it emerged in the humanities and social sciences, and why it emerged then. One possible explanation is that the humanities and social sciences were undergoing a profound shift in the 1980s, driven by the rise of postmodernism and poststructuralism. These intellectual movements, which emphasized the fragmented and provisional nature of knowledge, created a fertile ground for the kind of moral absolutism that characterizes wokeness.\n",
        "\"\"\"\n",
        "\n",
        "# Compare the texts\n",
        "similarity_score, shared_words = compare_texts(text1, text2)\n",
        "\n",
        "# Print results\n",
        "print(f\"Text 1:\\n{text1[:100]}...\\n\")  # Preview first 100 chars\n",
        "print(f\"Text 2:\\n{text2[:100]}...\\n\")\n",
        "print(f\"Similarity score: {similarity_score:.2f}\")\n",
        "print(f\"Number of shared words: {len(shared_words)}\")\n",
        "print(f\"Sample of shared words (up to 10): {list(shared_words)[:10]}\")\n"
      ],
      "metadata": {
        "id": "c9YZpIoGoohW",
        "outputId": "84628d4d-278e-4cac-8c6b-0511e4a01fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "The word \"prig\" isn't very common now, but if you look up the definition, it will sound familiar. Go...\n",
            "\n",
            "Text 2:\n",
            "wokeness would need to explain why it emerged in the humanities and social sciences, and why it emer...\n",
            "\n",
            "Similarity score: 0.10\n",
            "Number of shared words: 24\n",
            "Sample of shared words (up to 10): ['and', 'a', 'it', 'these', 'sciences', 'to', 'why', 'social', 'in', 'humanities']\n"
          ]
        }
      ]
    }
  ]
}